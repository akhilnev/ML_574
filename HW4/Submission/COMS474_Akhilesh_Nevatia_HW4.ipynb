{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS 474 HW4\n",
    "## Akhilesh Nevatia, akhilnev@iastate.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### (a) Neural Network for Alphanumeric Classification\n",
    "\n",
    "1. **Input Dimensions (d)**:\n",
    "   - The input to the neural network is a 32x32 RGB image. Since each pixel has 3 channels (Red, Green, and Blue), the total number of input features is calculated as:\n",
    "   $d = 32 \\times 32 \\times 3 = 3,072$\n",
    "   - This represents the flattened form of the image, where each pixel's RGB values are arranged in a single vector of size 3,072.\n",
    "\n",
    "2. **Output Dimensions (d_o)**:\n",
    "   - The task is to classify the image into one of 36 alphanumeric classes: 26 letters (a-z) and 10 digits (0-9). Therefore, the output dimension is:\n",
    "   $d_o = 36$\n",
    "\n",
    "3. **Hidden Layer Dimensions (d_H)**:\n",
    "   - The dimensions of the hidden layers ($d_H$) are not determined by the problem and are typically chosen based on the desired complexity of the model. This choice depends on factors such as the network's ability to generalize and the computational resources available.\n",
    "\n",
    "4. **Activation Functions**:\n",
    "   - **Hidden Layer Activation ($g_H(\\cdot)$)**: A common choice for hidden layers is the Rectified Linear Unit (ReLU) activation function, given its efficiency and effectiveness in practice.\n",
    "   - **Output Layer Activation ($g_O(\\cdot)$)**: Since this is a classification problem with multiple classes, the softmax function is used to convert the network's outputs into probabilities over the 36 classes.\n",
    "\n",
    "### (b) Neural Network for Sentiment Analysis\n",
    "\n",
    "1. **Input Dimensions (d)**:\n",
    "   - The input to the neural network is a paragraph consisting of 128 tokens. Since tokens need to be converted into a numerical form, they are represented by embedding vectors of size $E$ (e.g., 300 for common word embeddings). The total input dimension is:\n",
    "   $d = 128 \\times E$\n",
    "   - For example, if the embedding size $E$ is 300, then:\n",
    "   $d = 128 \\times 300 = 38,400$\n",
    "\n",
    "2. **Output Dimensions (d_o)**:\n",
    "   - The goal is to determine whether the sentiment of the text is \"happy\" or \"sad,\" which is a binary classification problem. Therefore, the output dimension is:\n",
    "   $d_o = 2$\n",
    "\n",
    "3. **Hidden Layer Dimensions (d_H)**:\n",
    "   - The dimensions of the hidden layers $d_H$ are not fixed by the problem and can be selected based on the complexity needed to model the relationships in the text. This can be determined through experimentation and optimization.\n",
    "\n",
    "4. **Activation Functions**:\n",
    "   - **Hidden Layer Activation ($g_H(\\cdot)$)**: A common activation function for hidden layers is the Rectified Linear Unit (ReLU) due to its efficiency and effectiveness.\n",
    "   - **Output Layer Activation ($g_O(\\cdot)$)**: For binary classification, a softmax function is used to convert the output into probabilities. Alternatively, a sigmoid activation function may also be suitable, depending on the implementation.\n",
    "\n",
    "### (c) Neural Network for GPS Coordinate Prediction\n",
    "\n",
    "1. **Input Dimensions (d)**:\n",
    "   - The input to the neural network consists of 20 past GPS coordinate pairs. Each pair includes a latitude and a longitude value, so the total number of input features is:\n",
    "   $d = 20 \\times 2 = 40$\n",
    "\n",
    "2. **Output Dimensions (d_o)**:\n",
    "   - The goal is to predict the future GPS coordinate pair, which also includes a latitude and a longitude. Therefore, the output dimension is:\n",
    "   $d_o = 2$\n",
    "\n",
    "3. **Hidden Layer Dimensions (d_H)**:\n",
    "   - The dimensions of the hidden layers $d_H$ are not explicitly determined by the problem. They should be chosen based on the complexity required to capture the patterns in the GPS data and can vary based on experimentation.\n",
    "\n",
    "4. **Activation Functions**:\n",
    "   - **Hidden Layer Activation ($g_H(\\cdot)$)**: The Rectified Linear Unit (ReLU) is a common choice for hidden layers because of its efficiency and effectiveness in training.\n",
    "   - **Output Layer Activation ($g_O(\\cdot)$)**: Since this is a regression problem (predicting real-valued coordinates), a linear activation function is used to ensure that the network's outputs are continuous and unbounded, matching the nature of GPS coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### (a) Code Below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Average Loss: 0.2780\n",
      "Epoch [20/100], Average Loss: 0.2579\n",
      "Epoch [30/100], Average Loss: 0.2088\n",
      "Epoch [40/100], Average Loss: 0.2075\n",
      "Epoch [50/100], Average Loss: 0.1668\n",
      "Epoch [60/100], Average Loss: 0.1402\n",
      "Epoch [70/100], Average Loss: 0.1016\n",
      "Epoch [80/100], Average Loss: 0.0876\n",
      "Epoch [90/100], Average Loss: 0.0771\n",
      "Epoch [100/100], Average Loss: 0.0721\n",
      "Mean Squared Error: 13.75115582331022\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data/Real_estate.csv')\n",
    "\n",
    "# Select features and target\n",
    "X = data.iloc[:, 1:7].values  # Features\n",
    "y = data.iloc[:, 7].values    # Target\n",
    "\n",
    "# Normalize both features AND target\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_normalized = X_scaler.fit_transform(X)\n",
    "y_normalized = y_scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_normalized, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(6, 28)\n",
    "        self.hidden2 = nn.Linear(28, 29)\n",
    "        self.hidden3 = nn.Linear(29, 30)\n",
    "        self.output = nn.Linear(30, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# After training loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get predictions\n",
    "    y_pred_normalized = model(X_tensor)\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_normalized.numpy())\n",
    "    \n",
    "    # Compute MSE on original scale\n",
    "    mse = np.mean((y - y_pred.ravel())**2)\n",
    "    print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Hidden Layers and Units\n",
    "- 3 hidden layers:\n",
    "  - Layer 1: 28 units\n",
    "  - Layer 2: 29 units\n",
    "  - Layer 3: 30 units\n",
    "\n",
    "### (c) Training Parameters\n",
    "- Learning Rate: 0.01\n",
    "- Batch Size: 32\n",
    "- Training Epochs: 100\n",
    "\n",
    "### (d) Number of Parameters\n",
    "Total: 1,968 parameters\n",
    "- Hidden Layer 1: 196 (6×28 + 28 bias)\n",
    "- Hidden Layer 2: 841 (28×29 + 29 bias)\n",
    "- Hidden Layer 3: 900 (29×30 + 30 bias)\n",
    "- Output Layer: 31 (30×1 + 1 bias)\n",
    "\n",
    "### (e) Model Comparison\n",
    "MSE Comparison:\n",
    "- Multiple Linear Regression: 77.53\n",
    "- Neural Network: 13.75115582331022\n",
    "\n",
    "The Neural Network performs better due to:\n",
    "1. Ability to capture non-linear relationships through ReLU activations\n",
    "2. Adaptive learning through Adam optimizer\n",
    "3. Multiple layers enabling complex pattern recognition\n",
    "\n",
    "The significantly lower MSE (~6x improvement) indicates that house prices have non-linear relationships with input features, making the neural network more suitable than linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "\n",
    "### (a) Accuracy of the model on the test images: 93.90%\n",
    "\n",
    "### (b) New accuracy of the model on the test images: 86.85%\n",
    "\n",
    "### Code changed: ( Attached below )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only modified code attached ( Swapping train and test data ) \n",
    "train_dataset = TensorDataset(test_images_tensor.unsqueeze(1), test_labels_tensor)  # Using test data for training\n",
    "test_dataset = TensorDataset(train_images_tensor.unsqueeze(1), train_labels_tensor)  # Using train data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading the data, modify y_train and y_test to use mod 2\n",
    "y_train = [y % 2 for y in y_train]\n",
    "y_test = [y % 2 for y in y_test]\n",
    "\n",
    "# When creating the model, change the output layer from 10 to 2 classes\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 2)  # Changed from 10 to 2 outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Minimum model change needed:**\n",
    "   - Change output layer from 10 to 2 nodes since we now have binary classification (mod 2)\n",
    "\n",
    "2. **One-hot encoded outcomes:**\n",
    "   - Even digits (0,2,4,6,8) → [1,0] \n",
    "   - Odd digits (1,3,5,7,9) → [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) \n",
    "#### Code attached below to modify training dataset to only have 10 of 3 labels and 10 of 9 labels : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added code filtering dataset using a counter to only include 10 of each label ( ADDITION DONE HERE )\n",
    "# After loading the data but before creating tensors\n",
    "count_3 = 0\n",
    "count_9 = 0\n",
    "filtered_x_train = []\n",
    "filtered_y_train = []\n",
    "\n",
    "# Iterate through training data\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 3 and count_3 < 10:\n",
    "        filtered_x_train.append(x_train[i])\n",
    "        filtered_y_train.append(y_train[i])\n",
    "        count_3 += 1\n",
    "    elif y_train[i] == 9 and count_9 < 10:\n",
    "        filtered_x_train.append(x_train[i])\n",
    "        filtered_y_train.append(y_train[i])\n",
    "        count_9 += 1\n",
    "    elif y_train[i] != 3 and y_train[i] != 9:\n",
    "        filtered_x_train.append(x_train[i])\n",
    "        filtered_y_train.append(y_train[i])\n",
    "\n",
    "# Replace original training data with filtered data\n",
    "x_train = filtered_x_train\n",
    "y_train = filtered_y_train\n",
    "\n",
    "#..... Code inbetween ......\n",
    "\n",
    "# Handling length of train data used below: ( MODIFICATIONS DONE HERE )\n",
    "random_images = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(0, len(x_train) - 1) # Changed to handle length of x_train change! \n",
    "    random_images.append((x_train[r], 'training image [' + str(r) + '] = ' + str(y_train[r])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Accuracy of model is 76.06% (Considerable decrease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Improvements for Imbalanced Dataset\n",
    "\n",
    "1. **Data Augmentation Techniques**:\n",
    "   - Apply rotations, shifts, and small distortions to the limited '3' and '9' samples\n",
    "   - This would create more training examples while preserving the digit characteristics\n",
    "   - Could use techniques like RandomRotation, RandomShift, or RandomZoom\n",
    "\n",
    "2. **Class Weighting**:\n",
    "   - Assign higher weights to the underrepresented classes (3 and 9) in the loss function\n",
    "   - This would make misclassifications of 3s and 9s more costly during training\n",
    "\n",
    "3. **Architecture Changes**:\n",
    "   - Add dropout layers to prevent overfitting on the limited examples\n",
    "   - Use batch normalization to stabilize training\n",
    "   - Consider a smaller network that's less likely to overfit\n",
    "\n",
    "4. **Training Strategy**:\n",
    "   - Implement early stopping to prevent overfitting\n",
    "   - Use a lower learning rate to find better local minima\n",
    "   - Employ cross-validation to better utilize the limited examples\n",
    "\n",
    "5. **Sampling Techniques**:\n",
    "   - Use SMOTE or other oversampling techniques to generate synthetic examples of 3s and 9s\n",
    "   - Implement batch sampling strategies to ensure each batch contains a mix of all classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "\n",
    "### Hidden Layer Design\n",
    "We use two hidden units to map `x_i` to specific activation patterns `a_H(x_i)`:\n",
    "\n",
    "#### Hidden Unit 1\n",
    "- **Condition**: Activates when `x >= 3`\n",
    "- **Weight**: `ω_H1 = 1`\n",
    "- **Bias**: `b_H1 = -3`\n",
    "- **Calculation**: `z_H1 = ω_H1 * x + b_H1`\n",
    "\n",
    "#### Hidden Unit 2\n",
    "- **Condition**: Activates when `x <= 3`\n",
    "- **Weight**: `ω_H2 = -1`\n",
    "- **Bias**: `b_H2 = 3`\n",
    "- **Calculation**: `z_H2 = ω_H2 * x + b_H2`\n",
    "\n",
    "### Calculating Activations for Each Data Point\n",
    "| x_i | y_i | z_H1 | a_H1 | z_H2 | a_H2 | a_H(x_i) |\n",
    "|-----|-----|------|------|------|------|----------|\n",
    "| -1  | 0   | -4   | 0    | 4    | 1    | [0, 1]   |\n",
    "| 1   | 0   | -2   | 0    | 2    | 1    | [0, 1]   |\n",
    "| 3   | 1   | 0    | 1    | 0    | 1    | [1, 1]   |\n",
    "| 5   | 0   | 2    | 1    | -2   | 0    | [1, 0]   |\n",
    "\n",
    "### Output Layer Design\n",
    "We need to find weights `ω_o` and bias `b_o` such that:\n",
    "- For `a_H = [0, 1]` (when `y_i = 0`): `z_o = ω_o1 * 0 + ω_o2 * 1 + b_o < 0`\n",
    "- For `a_H = [1, 0]` (when `y_i = 0`): `z_o = ω_o1 * 1 + ω_o2 * 0 + b_o < 0`\n",
    "- For `a_H = [1, 1]` (when `y_i = 1`): `z_o = ω_o1 * 1 + ω_o2 * 1 + b_o >= 0`\n",
    "\n",
    "#### Choosing Weights and Bias\n",
    "- **Output Weights**: `ω_o = [1, 1]`\n",
    "- **Output Bias**: `b_o = -1.5`\n",
    "\n",
    "### Verifying the Output\n",
    "| a_H(x_i) | z_o                            | u_o(x_i) | y_i |\n",
    "|----------|---------------------------------|----------|-----|\n",
    "| [0, 1]   | 0 + 1 * 1 - 1.5 = -0.5          | 0        | 0   |\n",
    "| [1, 0]   | 1 * 1 + 0 - 1.5 = -0.5          | 0        | 0   |\n",
    "| [1, 1]   | 1 * 1 + 1 * 1 - 1.5 = 0.5       | 1        | 1   |\n",
    "| [1, 0]   | 1 * 1 + 0 - 1.5 = -0.5          | 0        | 0   |\n",
    "\n",
    "---\n",
    "\n",
    "### Final Weights and Biases\n",
    "- **Hidden Layer Weights**: `ω_H = [1, -1]`\n",
    "- **Hidden Layer Biases**: `b_H = [-3, 3]`\n",
    "- **Output Layer Weights**: `ω_o = [1, 1]`\n",
    "- **Output Layer Bias**: `b_o = -1.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN ( CONVOLUTIONAL NEURAL NETWORK )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### (a) Code attached below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACvCAYAAACVbcM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVqUlEQVR4nO3daXAU1dfH8TNgTBAIgSgilARjpCACooILBgkKhiUClhG1XEAtd6MiiDtBpVALwhoQcUGRKjdkcRdLgiubC1pRojECAmrYJIBCENPPC//moXMap53MTU/3fD9VvLi/3O45xlsdDp3bHbIsyxIAAAAAiLIGXhcAAAAAIJhoNgAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjKDZAAAAAGAEzQYAAAAAI+K+2Vi/fr2EQiGZOHFi1M65bNkyCYVCsmzZsqidE8HE+oOXWH/wGmsQXmL91Q9fNhvPPvushEIh+eyzz7wuxZjNmzfL0KFDJSUlRZKTk2Xw4MHy448/el0WhPUHbwV9/X333XcyYsQI6dGjhyQlJUkoFJL169d7XRYOEvQ1KMI1MJYFff0F8Rp4mNcFQNuzZ4/07t1bKisr5d5775WEhASZPHmy9OrVS9asWSOpqalel4gAY/3BS8uXL5dp06ZJZmamdOzYUdasWeN1SYgzXAPhpSBeA2k2YtDMmTOlrKxMVq1aJd27dxcRkf79+0unTp2ksLBQxo8f73GFCDLWH7w0aNAg2blzpzRt2lQmTpwYiB+08BeugfBSEK+Bvvw1Kjf2798vY8aMkVNPPVWaNWsmjRs3lp49e0pxcfEhj5k8ebKkpaVJo0aNpFevXlJSUqLmlJaWSl5enrRo0UKSkpKkW7du8tprr4Wt548//pDS0lLZtm1b2Lnz58+X7t2711zkREQ6dOgg5557rrz88sthj4f3WH/wkp/XX4sWLaRp06Zh5yG2+XkNcg30Pz+vvyBeAwPbbOzatUueeuopyc7Olscee0zGjh0rW7dulZycHMcuce7cuTJt2jS5+eab5Z577pGSkhI555xzpKKiombON998I2eccYasXbtW7r77biksLJTGjRvLkCFDZOHChf9az6pVq6Rjx45SVFT0r/Oqq6vl66+/lm7duqmvnXbaaVJeXi67d+92902AZ1h/8JJf1x+Cw69rkGtgMPh1/QWW5UNz5syxRMRavXr1IeccOHDAqqqqsmW//fabdfTRR1tXX311TbZu3TpLRKxGjRpZmzZtqslXrlxpiYg1YsSImuzcc8+1OnfubO3bt68mq66utnr06GGdcMIJNVlxcbElIlZxcbHKCgoK/vW/bevWrZaIWA899JD62owZMywRsUpLS//1HDCL9cf681KQ119tEyZMsETEWrdu3X86DmYFeQ1yDYx9QV5/tQXlGhjYOxsNGzaUww8/XET+/peKHTt2yIEDB6Rbt27yxRdfqPlDhgyRNm3a1IxPO+00Of300+Wtt94SEZEdO3bI0qVLZejQobJ7927Ztm2bbNu2TbZv3y45OTlSVlYmmzdvPmQ92dnZYlmWjB079l/r3rt3r4iIJCYmqq8lJSXZ5iB2sf7gJb+uPwSHX9cg18Bg8Ov6C6rANhsiIs8995x06dJFkpKSJDU1VY466ih58803pbKyUs094YQTVNa+ffuax4398MMPYlmWPPDAA3LUUUfZ/hQUFIiIyJYtW+pcc6NGjUREpKqqSn1t3759tjmIbaw/eMmP6w/B4sc1yDUwOPy4/oIqsE+jmjdvngwfPlyGDBkid955p7Rs2VIaNmwojzzyiJSXl//n81VXV4uIyKhRoyQnJ8dxTkZGRp1qFvl7Y1BiYqL88ssv6mv/ZK1bt67z58As1h+85Nf1h+Dw6xrkGhgMfl1/QRXYZmP+/PmSnp4uCxYskFAoVJP/04HWVlZWprLvv/9e2rVrJyIi6enpIiKSkJAgffr0iX7B/9OgQQPp3Lmz48tqVq5cKenp6YF7SkEQsf7gJb+uPwSHX9cg18Bg8Ov6C6rA/hpVw4YNRUTEsqyabOXKlbJ8+XLH+YsWLbL9vt2qVatk5cqV0r9/fxERadmypWRnZ8sTTzzh+C8eW7du/dd6/stjz/Ly8mT16tW2i913330nS5culYsuuijs8fAe6w9e8vP6QzD4eQ1yDfQ/P6+/IPL1nY1nnnlG3nnnHZXfdtttkpubKwsWLJALLrhABg4cKOvWrZNZs2ZJZmam7NmzRx2TkZEhWVlZcuONN0pVVZVMmTJFUlNTZfTo0TVzZsyYIVlZWdK5c2e59tprJT09XSoqKmT58uWyadMm+eqrrw5Z66pVq6R3795SUFAQdoPQTTfdJE8++aQMHDhQRo0aJQkJCTJp0iQ5+uijZeTIke6/QTCK9QcvBXX9VVZWyvTp00VE5JNPPhERkaKiIklJSZGUlBS55ZZb3Hx7UA+Cuga5BvpDUNdfIK+BHjwBq87+eezZof5s3LjRqq6utsaPH2+lpaVZiYmJ1sknn2y98cYb1rBhw6y0tLSac/3z2LMJEyZYhYWF1rHHHmslJiZaPXv2tL766iv12eXl5daVV15ptWrVykpISLDatGlj5ebmWvPnz6+ZE43Hnm3cuNHKy8uzkpOTrSZNmli5ublWWVlZpN8yRBHrD14K+vr7pyanPwfXDu8EfQ1aFtfAWBb09RfEa2DIsg66xwQAAAAAURLYPRsAAAAAvEWzAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAETQbAAAAAIxw/VK/g1/3Dvyjvp6czPqDk/p8cjdrEE64BsJLrD94ye36484GAAAAACNoNgAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjKDZAAAAAGAEzQYAAAAAI2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYATNBgAAAAAjaDYAAAAAGHGY1wUA+NvUqVNVduutt6qspKREZbm5uSrbsGFDdAoDACBgMjMzbWOnn6PXXXedylavXq2yL7/8MuznTZkyRWX79+8Pe1wQcGcDAAAAgBE0GwAAAACMoNkAAAAAYATNBgAAAAAjQpZlWa4mhkKma4lL999/v8oefPBBlTVoYO8Ls7Oz1ZwPPvgganW55XL51FkQ11+7du1s488//1zNSUlJUZnT93zgwIEqe/fddyOuzS/qa/2JBHMNtm/f3jZOSEhQc84++2yVzZw5U2XV1dXRK8zB4sWLbeNLLrlEzfFisyXXwOhxWn89evRQ2fjx41V21llnGakp1rH+3Ln++utVNnHiRNu4SZMmRms455xzVFZcXGz0M01zu/64swEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBFsEK9Hw4cPV9n06dNVlpSUpLLaG8R79+6t5nz44YeRFxchNqdFrnHjxrbxvHnz1JxBgwapjA3i/48N4s5OPPFElTldfy666CLbuPZ1RkSkdevWKnP6XtTn/wsRkblz56rs9ttvV9muXbuM1sE1MHqOPPJIlW3ZskVlv/76q8pOOeUUV/OChvXnTosWLVS2du1a27hly5ZGa9i5c6fKLr74YpUtWbLEaB3RxAZxAAAAAJ6i2QAAAABgBM0GAAAAACMO87qAeJKWlqYyp/0ZiA+///67bbxhwwaPKkHQPPLIIyobMGCAB5WYc+WVV6rs6aefVtknn3xSH+WgHrVq1cpVFg97NuDOjh07VFZQUGAbFxYWqjlHHHGEyn766SeVtW3bNmwNTi/p7devn8r8tGfDLe5sAAAAADCCZgMAAACAETQbAAAAAIyg2QAAAABgBBvEDerTp49tnJ+f7+q40tJSleXm5trGFRUVkReGmFB7s9hJJ53kTSEInPfee09lbjaIO71AzWnTtdPL/6qrq8Oev0ePHirr1atX2OOAg/n9BXOIDbNmzbKNb7jhBjXH6edyNF8UWlRUFLVzxTLubAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYAQbxKMkKytLZXPmzLGNmzVr5upcEyZMUBlvlw6e2m8mdfMG0kPp3r27ymo/aIA1FD8ef/xxlS1atCjscX/++afKovkW5uTkZJWVlJSorHXr1mHP5fTf89lnn0VUF/zFsiyVJSUleVAJgmTcuHEqu++++1TWtWvXqH3m4YcfHrVzxTLubAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYAQbxKNk2LBhKnOzyXHZsmUqmzt3bjRKQoz7+eefbeNnn31WzRk7dqyrcznN27lzp20cL28qhciBAwdUtnHjRg8qscvJyVFZ8+bNIzrXpk2bVFZVVRXRueB/3bp1U9mKFSs8qAR+NX/+fJV9/PHHKluyZInKOnfuHNFnOm1Kz8vLi+hcsYw7GwAAAACMoNkAAAAAYATNBgAAAAAjaDYAAAAAGMEG8QgceeSRKrv66qtVVl1dbRvX3rAr4rw5CPHp4YcfVpnbDeJALLrkkkts42uvvVbNadSoUUTnHjNmTETHIXY5PdigsrJSZc2aNVPZ8ccfb6QmxI/LLrtMZSeddJLKOnXqFLXPdNqAHkTc2QAAAABgBM0GAAAAACNoNgAAAAAYwZ6NMNq1a6eyV199NaJzTZ8+XWXFxcURnQvxoUED/e8BtfcCAfXN6Xeb7777bpVlZGTYxgkJCRF/5po1a2zjP//8M+JzITY57Wv86KOPVJabm1sP1SBIOnTooLKFCxfaxrWvVyIihx1m9q/Jr732mtHzxwrubAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYAQbxMPo16+fyrp06eLq2Pfff982njp1alRqQvxw2gxuWZYHlcBPnB5sccUVV6isT58+EZ0/KytLZZGuy127dqnMabP5W2+9ZRvv3bs3os8DEH86duyosuOOO842Nr0Z3MmIESNUlp+fX+91mMadDQAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjGCD+EGGDBmiskcffdTVsR9//LHKhg0bZhtXVlZGVBcAHEqnTp1U5vRW2rZt29ZHOf+Z0xuiZ8+e7UEl8LPU1FSvS0AMq/22cBGR0aNH28aPPfaYmpOUlGSsJhGRY445xuj5YwV3NgAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMCKuN4jXfsvuq6++GvG5fvzxR5VVVFREfD4AiFQoFHKVRapBA/3vVE5vu3cjNzdXZf3791fZ22+/HdH5ER8GDRrkdQnwmWnTptnGZWVlak5KSoqrczm9fbyoqMg2Tk5Odl9cwHBnAwAAAIARNBsAAAAAjKDZAAAAAGAEzQYAAAAAI+J6g/hdd91lG0e6wVHE/ZvGgf+iLhtxzz77bNu49mY1BENJSYnKsrOzVXb55Zer7N1337WN9+3bF7W6RESuueYa2zg/Pz+q50fwFRcXq8zpoQJAXdXlIRROD+DIyMiwjceMGaPmdO3aVWVpaWkq27BhQ8S1xQLubAAAAAAwgmYDAAAAgBE0GwAAAACMCFmWZbmaGMUXQnnB6ffiar/Er23btq7OtXjxYpXl5eVFVJffuVw+deb39Repv/76S2WRfs+7dOmism+//Taic8WK+lp/IvG7BuuiWbNmtvH27dtdHXf++eerLFZf6sc10KwLL7xQZa+88orK9u7dq7LMzEzb2O+/9+6E9RcbEhMTVeZmD1xpaanK+vbtq7JNmzZFVphhbtcfdzYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADAibl7qt2TJEpU1b9487HErVqxQ2fDhw6NREhDWrFmzVHb99ddHdK7rrrtOZbfffntE5wLcyMnJ8boE+NyBAwdczXPawOy0aRcwYdy4cREd9/TTT6ssVjeD1wV3NgAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMCJuNoinpqaqrLq6OuxxM2fOVNmePXuiUhMQjtPbRRE/EhISbOPzzjtPzVm6dKnKnN6mbNpVV12lsqlTp9Z7HQiWxYsXq8zputihQweV1X4Axk033RS1umBe7b+3zZkzR8154YUXXGXRdMwxx6jM6QEsbixYsKCu5fgCdzYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADAikBvEnTYRNWgQWV/16aef1rUcIGLTp09XWX5+vsqOP/74sOe67bbbXJ2/vLzcZXWIpqysLJXdd999tnHfvn3VnOOOO05lGzdujFpdLVq0UNmAAQNUNmnSJJUdccQRYc/vtJl93759LqtDPFqyZInK2rRpo7I77rijPsqBIdOmTbONzz//fDWnffv2Kvv5559VtnnzZpX98MMPtvGpp57q6vyjR49WWXJysspqKywsVJlTrUHEnQ0AAAAARtBsAAAAADCCZgMAAACAEb7fs9G1a1eV9enTR2VOL/Dbv3+/bTxjxgw1p6KiIvLiAAO++eYblaWnp4c9zs1LLOGdoqIilXXq1CnscU6/P7x79+6o1CTivE/klFNOUZllWWHPtWzZMpU9/vjjKisuLnZXHPA/Tuuv9s94+EvtPYVO+9POPPNMlTldZ9avX6+yb7/91jbu2bOnmtO0adMwVf7Naf3VfvlkQUGBmhMv+9O4swEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBG+3yCekpKislatWrk6tvZLXkaNGhWNkgCjZs+erTKnlx0hPtx4441elyAiIlu2bFHZ66+/bhs7vVgyXjZIwiynl6oNHjzYNl64cGF9lYMoWLFihW28fPlyNef5559X2cyZM1XWrl07V1mkfvvtN5VlZmZG7fx+x50NAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACM8P0GcSDe1H7rqYjI2rVrVdaxY8f6KAdRMnz4cJXl5+fbxsOGDTNaQ3l5ucr++OMPlX300Ucqc3pwQUlJSXQKAw4ydOhQlVVVVanM6boI/xo5cqTKEhMTVdakSRNX5zv55JNt40svvdTVcZWVlSrr27evq2PjFXc2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwImRZluVqYihkupaIOL0t/KWXXlJZVlaWytatW2cbZ2RkRK+wOOFy+dRZrK4/eKu+1p+IN2uw9uZHp03k48aNU1nz5s1VtmjRIpW99957tvHixYvVnF9//TVMlfGNa2D9e/HFF1Xm9ECMQYMG2cYbNmwwVpNXWH/wktv1x50NAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACM8P0GcXiLzWnwUtA3iCP2cQ2El1h/8BIbxAEAAAB4imYDAAAAgBE0GwAAAACMoNkAAAAAYATNBgAAAAAjaDYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAETQbAAAAAIyg2QAAAABgBM0GAAAAACNClmVZXhcBAAAAIHi4swEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYATNBgAAAAAjaDYAAAAAGEGzAQAAAMCI/wMagMvuhjKRUQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Modify the data filtering to remap labels\n",
    "filtered_data = [(image, 0 if label == 1 else 1) for image, label in mnist_dataset if label in [1, 3]]\n",
    "\n",
    "# Limit the size of the dataset for computational efficiency (e.g., 1000 samples)\n",
    "subset_size = 1000\n",
    "custom_dataset = filtered_data[:subset_size]\n",
    "\n",
    "# Extract images and labels for plotting\n",
    "images, labels = zip(*custom_dataset)\n",
    "\n",
    "# Plot a few examples from the customized dataset\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {labels[i]}')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Code Below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model trained for 1 epoch.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prepare the data loader\n",
    "data_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (1 epoch)\n",
    "model.train()\n",
    "for images, labels in data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"CNN model trained for 1 epoch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Code for ResNet Model trained below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhileshnevatia/ML_574/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/akhileshnevatia/ML_574/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/akhileshnevatia/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet model trained for 1 epoch.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Load a pre-trained ResNet model and adjust for MNIST\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 2)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (1 epoch)\n",
    "resnet.train()\n",
    "for images, labels in data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = resnet(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"ResNet model trained for 1 epoch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Code for Feedforward NN model with D below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward NN model trained for 1 epoch.\n"
     ]
    }
   ],
   "source": [
    "# Define a simple Feedforward Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the images\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "nn_model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (1 epoch)\n",
    "nn_model.train()\n",
    "for images, labels in data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = nn_model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Feedforward NN model trained for 1 epoch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "### (a) Number of Parameters\n",
    "- **Weights per Kernel**: Each kernel has dimensions 5x5 and spans 3 input channels (for RGB). Thus, the number of weights per kernel is:\n",
    "  - 5 * 5 * 3 = 75\n",
    "- **Bias per Kernel**: 1 bias term for each kernel\n",
    "- **Total Parameters per Kernel**: 75 (weights) + 1 (bias) = 76\n",
    "- **Number of Kernels**: 10\n",
    "- **Total Parameters in the Layer**: 76 * 10 = 760\n",
    "\n",
    "**Answer**: The convolutional layer has **760 parameters** in total.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Minimum Image Size for Compatibility\n",
    "- **Given**:\n",
    "  - Kernel Size: 5\n",
    "  - Stride: 1\n",
    "  - Padding: 0\n",
    "- **Output Size Formula**: (Input Size - Kernel Size) / Stride + 1\n",
    "- **Setting Output Size to 1**:\n",
    "  - (Input Size - 5) / 1 + 1 = 1\n",
    "  - Simplifying: Input Size - 5 = 0, so Input Size = 5\n",
    "\n",
    "**Answer**: The minimum image size is **5x5x3** (height 5, width 5, and 3 channels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: \n",
    "\n",
    "Key Problem Addressed by ResNet\n",
    "\n",
    "- **Problem Addressed**: ResNet (Residual Networks) aims to solve the problem of the **vanishing/exploding gradient** in very deep neural networks, which makes training difficult.\n",
    "- **Explanation**:\n",
    "  - As neural networks become deeper, gradients can become extremely small (vanishing) or very large (exploding) during backpropagation.\n",
    "  - This issue leads to poor convergence or models that fail to learn effectively.\n",
    "\n",
    "- **Key Concept**: **Residual Learning**\n",
    "  - ResNet introduces **residual connections** (or skip connections), which allow the model to learn identity mappings more easily by bypassing one or more layers.\n",
    "  - These skip connections make it easier for the network to learn transformations and ensure better gradient flow.\n",
    "\n",
    "- **Impact**:\n",
    "  - Enables the training of much deeper networks (e.g., 50, 101, or even more layers) without suffering from severe degradation in performance.\n",
    "  - Improves accuracy and stability in deep architectures.\n",
    "\n",
    "**Summary**: ResNet addresses the challenge of training deep networks by using residual connections to improve gradient flow and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: \n",
    "\n",
    "Key Problem Addressed by Dropout\n",
    "\n",
    "- **Problem Addressed**: Dropout aims to tackle the problem of **overfitting** in deep neural networks.\n",
    "- **Explanation**:\n",
    "  - Overfitting occurs when a model learns to perform well on the training data but fails to generalize to unseen data.\n",
    "  - This happens because the model becomes too complex, capturing noise and unnecessary details from the training set.\n",
    "\n",
    "- **Key Concept**: **Regularization Technique**\n",
    "  - Dropout is a regularization method that randomly \"drops out\" (sets to zero) a fraction of neurons during each training iteration.\n",
    "  - By doing this, the network becomes less dependent on specific neurons, forcing it to learn more robust and generalizable features.\n",
    "\n",
    "- **Impact**:\n",
    "  - Reduces the likelihood of overfitting and improves the network's ability to generalize to new data.\n",
    "  - Acts as a form of ensemble learning, as different subsets of the network are trained in each iteration.\n",
    "\n",
    "**Summary**: Dropout helps prevent overfitting by randomly dropping neurons during training, which promotes the learning of more generalized features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
